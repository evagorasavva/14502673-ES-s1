{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook implements a machine learning pipeline for predicting student academic dropout rates.\n",
    "The analysis includes:\n",
    "- Data preprocessing\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Outlier detection and handling\n",
    "- Feature engineering\n",
    "- Model training and evaluation\n",
    "- Hyperparameter tuning\n",
    "\n",
    "This work is aimed at improving the understanding of factors contributing to academic dropouts and creating predictive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: xgboost in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\evago\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\evago\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Evago\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import joblib\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "\n",
    "class AcademicDropoutAnalysis:\n",
    "    def __init__(self, data_path, delimiter=',', random_state=42, num_features=5):\n",
    "        self.random_state = random_state\n",
    "        self.delimiter = delimiter\n",
    "        self.num_features = num_features\n",
    "        self.best_params = {}\n",
    "        self.setup_logging()\n",
    "        self.logger.info(\"Initializing Academic Dropout Analysis pipeline\")\n",
    "        self.load_data(data_path)\n",
    "        self.perform_eda()\n",
    "        self.detect_and_handle_outliers()\n",
    "        self.prepare_data()\n",
    "        self.identify_features()\n",
    "        self.create_preprocessing_pipeline()\n",
    "        self.split_data()\n",
    "        self.train_and_evaluate_models()\n",
    "        self.tune_models()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Set up logging configuration.\"\"\"\n",
    "        log_dir = 'logs'\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        log_file = os.path.join(log_dir, f'academic_dropout_analysis_{timestamp}.log')\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        \"\"\"Load data and perform initial cleanup.\"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(data_path, sep=self.delimiter)\n",
    "            # Clean column names: lower, strip spaces and quotes\n",
    "            self.data.columns = self.data.columns.str.lower().str.strip().str.replace('\"', '')\n",
    "            self.logger.info(f\"Data loaded successfully with shape: {self.data.shape}\")\n",
    "            self.logger.info(f\"Columns in the dataset: {self.data.columns.tolist()}\")\n",
    "\n",
    "            # Drop columns with >50% missing values\n",
    "            missing_thresh = len(self.data) * 0.5\n",
    "            initial_columns = self.data.shape[1]\n",
    "            self.data = self.data.dropna(axis=1, thresh=missing_thresh)\n",
    "            dropped_columns = initial_columns - self.data.shape[1]\n",
    "            self.logger.info(f\"Dropped {dropped_columns} columns with >50% missing values\")\n",
    "\n",
    "         \n",
    "            self._remove_constant_features()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _remove_constant_features(self):\n",
    "        \"\"\"Remove constant and quasi-constant features.\"\"\"\n",
    "        nunique = self.data.nunique()\n",
    "        constant_features = nunique[nunique == 1].index.tolist()\n",
    "        quasi_constant_features = []  \n",
    "        self.data.drop(columns=constant_features + quasi_constant_features, inplace=True)\n",
    "        \n",
    "        self.logger.info(f\"Removed {len(constant_features)} constant features\")\n",
    "\n",
    "    def perform_eda(self):\n",
    "        \"\"\"Perform Exploratory Data Analysis (EDA).\"\"\"\n",
    "        eda_dir = 'eda'\n",
    "        os.makedirs(eda_dir, exist_ok=True)\n",
    "\n",
    "        self.logger.info(\"Starting Exploratory Data Analysis (EDA)\")\n",
    "\n",
    "        # Identify the target column\n",
    "        target_col = 'target'\n",
    "        if target_col not in self.data.columns:\n",
    "            self.logger.error(\"No 'Target' column found in dataset. Ensure the target column is named 'Target'.\")\n",
    "            raise ValueError(\"Target column ('Target') not found.\")\n",
    "\n",
    "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        # Exclude target from numeric for plotting distributions\n",
    "        numeric_cols = [col for col in numeric_cols if col != target_col]\n",
    "        categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        # Function to create a safe filename\n",
    "        def safe_filename(name):\n",
    "            return name.replace('/', '_').replace('\\\\', '_').replace(' ', '_')\n",
    "\n",
    "        # Plot distribution of numerical features\n",
    "        for col in numeric_cols:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.histplot(self.data[col].dropna(), kde=True, bins=30)\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            filename = f'distribution_{safe_filename(col)}.png'\n",
    "            plt.savefig(os.path.join(eda_dir, filename))\n",
    "            plt.close()\n",
    "\n",
    "        # Plot count of categorical features (limit to top 10 categories)\n",
    "        for col in categorical_cols:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            top_categories = self.data[col].value_counts().nlargest(10)\n",
    "            sns.barplot(x=top_categories.values, y=top_categories.index)\n",
    "            plt.title(f'Count of Top 10 Categories in {col}')\n",
    "            plt.xlabel('Count')\n",
    "            plt.ylabel(col)\n",
    "            filename = f'count_{safe_filename(col)}.png'\n",
    "            plt.savefig(os.path.join(eda_dir, filename))\n",
    "            plt.close()\n",
    "\n",
    "        # Correlation matrix for numerical features\n",
    "        if len(numeric_cols) > 1:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            corr_matrix = self.data[numeric_cols].corr()\n",
    "            sns.heatmap(corr_matrix, annot=False, fmt=\".2f\", cmap='coolwarm')\n",
    "            plt.title('Correlation Matrix')\n",
    "            plt.savefig(os.path.join(eda_dir, 'correlation_matrix.png'))\n",
    "            plt.close()\n",
    "\n",
    "        # Class distribution plot for Target\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.countplot(x=self.data[target_col])\n",
    "        plt.title('Class Distribution of Target')\n",
    "        plt.xlabel('Target')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig(os.path.join(eda_dir, 'class_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "        self.logger.info(\"EDA completed and plots saved in 'eda/' directory\")\n",
    "\n",
    "    def detect_and_handle_outliers(self, z_score_threshold=3, outlier_ratio_threshold=0.1):\n",
    "        \"\"\"Detect and handle outliers in numerical features using a Z-score method.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting outlier detection and handling\")\n",
    "        outlier_dir = 'outliers'\n",
    "        os.makedirs(outlier_dir, exist_ok=True)\n",
    "\n",
    "        target_col = 'target'\n",
    "        numeric_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        # Exclude target variable from outlier detection\n",
    "        if target_col in numeric_cols:\n",
    "            numeric_cols.remove(target_col)\n",
    "\n",
    "        features_to_drop = []\n",
    "        for col in numeric_cols:\n",
    "            # Drop Nan values for calculation\n",
    "            col_data = self.data[col].dropna()\n",
    "            if col_data.empty:\n",
    "                continue  \n",
    "\n",
    "            # Calculate Z-scores\n",
    "            z_scores = np.abs(stats.zscore(col_data))\n",
    "            outliers = z_scores > z_score_threshold\n",
    "            num_outliers = np.sum(outliers)\n",
    "            outlier_ratio = num_outliers / len(col_data)\n",
    "\n",
    "            self.logger.info(f\"Feature '{col}': Found {num_outliers} potential outliers out of {len(col_data)} (ratio={outlier_ratio:.2f})\")\n",
    "\n",
    "            if outlier_ratio_threshold is not None and outlier_ratio > outlier_ratio_threshold:\n",
    "                # Drop the entire feature if it has too many outliers\n",
    "                self.logger.info(f\"Dropping feature '{col}' due to excessive outliers (ratio={outlier_ratio:.2f} > {outlier_ratio_threshold})\")\n",
    "                features_to_drop.append(col)\n",
    "            else:\n",
    "                # Cap the outliers at 1st and 99th percentiles\n",
    "                if num_outliers > 0:\n",
    "                    lower_bound = self.data[col].quantile(0.01)\n",
    "                    upper_bound = self.data[col].quantile(0.99)\n",
    "                    self.data[col] = np.where(self.data[col] < lower_bound, lower_bound, self.data[col])\n",
    "                    self.data[col] = np.where(self.data[col] > upper_bound, upper_bound, self.data[col])\n",
    "                    self.logger.info(f\"Capped outliers in '{col}' to 1st and 99th percentiles\")\n",
    "\n",
    "                    # Plot boxplot after handling outliers\n",
    "                    plt.figure(figsize=(8, 4))\n",
    "                    sns.boxplot(x=self.data[col])\n",
    "                    plt.title(f'Boxplot of {col} After Outlier Handling')\n",
    "                    plt.savefig(os.path.join(outlier_dir, f'boxplot_{col}.png'))\n",
    "                    plt.close()\n",
    "\n",
    "        # Drop features identified for removal\n",
    "        if features_to_drop:\n",
    "            self.data.drop(columns=features_to_drop, inplace=True)\n",
    "            self.logger.info(f\"Dropped {len(features_to_drop)} features due to outlier issues: {features_to_drop}\")\n",
    "\n",
    "        self.logger.info(\"Outlier detection and handling completed. Boxplots saved in 'outliers/' directory\")\n",
    "\n",
    "    def prepare_data(self):\n",
    "            \"\"\"Prepare features and target variable.\"\"\"\n",
    "            # Identify target\n",
    "            target_col = 'target'\n",
    "            if target_col not in self.data.columns:\n",
    "                self.logger.error(\"No 'Target' column found in dataset.\")\n",
    "                raise ValueError(\"Target column ('Target') not found.\")\n",
    "\n",
    "            # Encode the target since it's multiclass categorical (dropout, enrolled, graduate)\n",
    "            self.target_encoder = LabelEncoder()\n",
    "            self.data[target_col] = self.target_encoder.fit_transform(self.data[target_col])\n",
    "            self.logger.info(f\"Encoded target classes: {self.target_encoder.classes_}\")\n",
    "\n",
    "            # Separate features and target\n",
    "            self.features = self.data.drop(columns=[target_col])\n",
    "            self.target = self.data[target_col]\n",
    "            self.logger.info(\"Features and target prepared.\")\n",
    "\n",
    "    def identify_features(self):\n",
    "        \"\"\"Identify numerical and categorical features.\"\"\"\n",
    "        \n",
    "        self.numeric_features = self.features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        # Identify categorical features as object or category\n",
    "        self.categorical_features = self.features.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        self.logger.info(f\"Identified {len(self.numeric_features)} numerical features: {self.numeric_features}\")\n",
    "        self.logger.info(f\"Identified {len(self.categorical_features)} categorical features: {self.categorical_features}\")\n",
    "\n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"Create preprocessing pipeline.\"\"\"\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, self.numeric_features),\n",
    "                ('cat', categorical_transformer, self.categorical_features)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\"Preprocessing pipeline created\")\n",
    "\n",
    "    def split_data(self):\n",
    "        \"\"\"Split data into training and testing sets.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.features, self.target,\n",
    "            test_size=0.2,\n",
    "            random_state=self.random_state,\n",
    "            stratify=self.target\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Split data into training ({X_train.shape}) and testing ({X_test.shape}) sets\")\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def train_and_evaluate_models(self):\n",
    "        \"\"\"Train and evaluate models.\"\"\"\n",
    "        # Defines the models and hyperparameter grids\n",
    "        self.models = {\n",
    "'LogisticRegression': {\n",
    "    'model': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=self.random_state,\n",
    "        multi_class='ovr'\n",
    "    ),\n",
    "    'params': {\n",
    "        'classifier__C': [0.01, 0.1, 1, 10],\n",
    "        'classifier__penalty': ['l2']\n",
    "    }\n",
    "},\n",
    "'SVC': {\n",
    "    'model': SVC(\n",
    "        probability=True,\n",
    "        random_state=self.random_state,\n",
    "        decision_function_shape='ovr'\n",
    "    ),\n",
    "    'params': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__gamma': ['scale', 'auto'],\n",
    "        'classifier__kernel': ['rbf', 'linear']\n",
    "    }\n",
    "},\n",
    "'XGBoost': {\n",
    "    'model': XGBClassifier(\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=self.random_state,\n",
    "    ),\n",
    "    'params': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__max_depth': [3, 5],\n",
    "        'classifier__learning_rate': [0.01, 0.1],\n",
    "        'classifier__subsample': [0.7, 0.8, 1.0],\n",
    "        'classifier__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "    }\n",
    "},\n",
    "'GradientBoosting': {\n",
    "    'model': GradientBoostingClassifier(\n",
    "        random_state=self.random_state\n",
    "    ),\n",
    "    'params': {\n",
    "        'classifier__n_estimators': [100, 200],\n",
    "        'classifier__learning_rate': [0.01, 0.1],\n",
    "        'classifier__max_depth': [3, 5],\n",
    "        'classifier__subsample': [0.7, 0.8, 1.0]\n",
    "    }\n",
    "},\n",
    "'KNeighbors': {\n",
    "    'model': KNeighborsClassifier(),\n",
    "    'params': {\n",
    "        'classifier__n_neighbors': [3, 5, 7],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "}\n",
    "            }\n",
    "\n",
    "\n",
    "        # Create directories for results and feature importance\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        feature_importance_dir = 'feature_importance'\n",
    "        os.makedirs(feature_importance_dir, exist_ok=True)\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        for name, config in self.models.items():\n",
    "            self.logger.info(f\"\\nTraining and evaluating {name}\")\n",
    "            pipeline = ImbPipeline([\n",
    "                ('preprocessing', self.preprocessor),\n",
    "                ('feature_selection', SelectKBest(score_func=f_classif, k=min(self.num_features, len(self.numeric_features) + len(self.categorical_features)))),\n",
    "                ('smote', SMOTE(random_state=self.random_state)),\n",
    "                ('classifier', config['model'])\n",
    "            ])\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=config['params'],\n",
    "                scoring='f1_weighted',\n",
    "                cv=cv,\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(self.X_train, self.y_train)\n",
    "            self.logger.info(f\"{name} Grid Search completed\")\n",
    "            self.logger.info(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "            self.logger.info(f\"Best cross-validation F1 score for {name}: {grid_search.best_score_:.3f}\")\n",
    "            \n",
    "            self.best_params[name] = grid_search.best_params_\n",
    "            \n",
    "            joblib.dump(grid_search.best_estimator_, os.path.join('results', f'{name}_best_pipeline.joblib'))\n",
    "            self.logger.info(f\"Saved {name} best pipeline\")\n",
    "\n",
    "            y_pred = grid_search.predict(self.X_test)\n",
    "\n",
    "            # For multiclass ROC AUC, need predict_proba\n",
    "            if hasattr(grid_search.best_estimator_.named_steps['classifier'], \"predict_proba\"):\n",
    "                y_pred_proba = grid_search.predict_proba(self.X_test)\n",
    "                roc_auc = roc_auc_score(self.y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "            else:\n",
    "                # If no predict_proba, skip ROC AUC or sets to None\n",
    "                roc_auc = None\n",
    "            \n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "\n",
    "            self.logger.info(f\"{name} Test Accuracy: {accuracy:.3f}\")\n",
    "            self.logger.info(f\"{name} Test F1 Score: {f1:.3f}\")\n",
    "            if roc_auc is not None:\n",
    "                self.logger.info(f\"{name} Test ROC AUC: {roc_auc:.3f}\")\n",
    "            else:\n",
    "                self.logger.info(f\"{name} Test ROC AUC: N/A (no predict_proba)\")\n",
    "\n",
    "            self.results.append({\n",
    "                'Model': name,\n",
    "                'CV_F1_Mean': round(grid_search.best_score_, 3),\n",
    "                'Test_Accuracy': round(accuracy, 3),\n",
    "                'Test_F1': round(f1, 3),\n",
    "                'Test_ROC_AUC': round(roc_auc, 3) if roc_auc is not None else None\n",
    "            })\n",
    "            \n",
    "            with open(os.path.join('results', f'{name}_report.txt'), 'w') as f:\n",
    "                f.write(f\"Classification Report for {name}:\\n\")\n",
    "                f.write(classification_report(self.y_test, y_pred, target_names=self.target_encoder.classes_))\n",
    "                f.write(f\"\\nBest cross-validation F1 score: {grid_search.best_score_:.3f}\")\n",
    "                f.write(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "            \n",
    "            # Feature importance\n",
    "            if name in ['RandomForest', 'GradientBoosting', 'XGBoost','SVC']:\n",
    "                try:\n",
    "                    preprocessor = grid_search.best_estimator_.named_steps['preprocessing']\n",
    "                    # Attempt to get feature names\n",
    "                    if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "                        all_feature_names = preprocessor.get_feature_names_out()\n",
    "                    else:\n",
    "                        #if can't get names\n",
    "                        all_feature_names = self.numeric_features + self.categorical_features\n",
    "                    \n",
    "                    feature_selector = grid_search.best_estimator_.named_steps['feature_selection']\n",
    "                    if hasattr(feature_selector, 'get_support'):\n",
    "                        selected_indices = feature_selector.get_support(indices=True)\n",
    "                        selected_features = [all_feature_names[i] for i in selected_indices]\n",
    "                    else:\n",
    "                        selected_features = all_feature_names\n",
    "                    \n",
    "                    classifier = grid_search.best_estimator_.named_steps['classifier']\n",
    "                    if hasattr(classifier, 'feature_importances_'):\n",
    "                        importances = classifier.feature_importances_\n",
    "                    elif hasattr(classifier, 'coef_'):\n",
    "                        importances = np.mean(np.abs(classifier.coef_), axis=0)\n",
    "                    else:\n",
    "                        self.logger.warning(f\"No feature importances for {name}.\")\n",
    "                        continue\n",
    "                    \n",
    "                    if len(importances) != len(selected_features):\n",
    "                        self.logger.warning(\"Mismatch in importances and features length.\")\n",
    "                        continue\n",
    "                    \n",
    "                    fi_df = pd.DataFrame({\n",
    "                        'Feature': selected_features,\n",
    "                        'Importance': importances\n",
    "                    }).sort_values(by='Importance', ascending=False)\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 8))\n",
    "                    sns.barplot(x='Importance', y='Feature', data=fi_df.head(20))\n",
    "                    plt.title(f'Feature Importances for {name}')\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(feature_importance_dir, f'feature_importance_{name}.png'))\n",
    "                    plt.close()\n",
    "                    \n",
    "                    fi_df.to_csv(os.path.join(feature_importance_dir, f'feature_importance_{name}.csv'), index=False)\n",
    "                    self.logger.info(f\"Feature importance analysis completed for {name}.\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error during feature importance analysis for {name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        results_df.to_csv(os.path.join('results', 'model_performance_initial.csv'), index=False)\n",
    "        self.logger.info(\"Initial model performance metrics saved.\")\n",
    "\n",
    "    def tune_models(self):\n",
    "        \"\"\"Tune the models further to achieve better performance using refined hyperparameter grids.\"\"\"\n",
    "        self.logger.info(\"\\nStarting the tuning of models for better performance.\")\n",
    "        \n",
    "        for name, config in self.models.items():\n",
    "            self.logger.info(f\"\\nTuning {name}\")\n",
    "            try:\n",
    "                best_pipeline_path = os.path.join('results', f'{name}_best_pipeline.joblib')\n",
    "                if not os.path.exists(best_pipeline_path):\n",
    "                    self.logger.warning(f\"Best pipeline for {name} not found. Skipping tuning for this model.\")\n",
    "                    continue\n",
    "                pipeline = joblib.load(best_pipeline_path)\n",
    "                \n",
    "                initial_best_params = self.best_params.get(name, {})\n",
    "                \n",
    "                refined_params = {}\n",
    "                for param, value in initial_best_params.items():\n",
    "                    # Only refine if value is numeric \n",
    "                    if 'subsample' in param or 'colsample_bytree' in param:\n",
    "                        base_values = [value]\n",
    "                        #Decreasing/increasing slightly if it does not exceed [0,1]\n",
    "                        for delta in [-0.05, 0.05]:\n",
    "                            new_val = value + delta\n",
    "                            if 0 < new_val <= 1:\n",
    "                                base_values.append(new_val)\n",
    "                        # Ensures unique and sorted\n",
    "                        base_values = sorted(set(base_values))\n",
    "                        refined_params[param] = base_values\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        # adjusted by +/- 1 \n",
    "                        candidates = [max(value-1, 1), value, value+1]\n",
    "                        # Remove duplicates and ensure positive values if needed\n",
    "                        candidates = [c for c in candidates if c > 0]\n",
    "                        refined_params[param] = sorted(set(candidates))\n",
    "                    else:\n",
    "                        # Non-numeric or parameters that shouldn't be refined\n",
    "                        refined_params[param] = [value]\n",
    "\n",
    "                # Remove parameters that ended up empty\n",
    "                refined_params = {k: v for k, v in refined_params.items() if len(v) > 0}\n",
    "                \n",
    "                # If no refined parameters, skip tuning\n",
    "                if not refined_params:\n",
    "                    self.logger.warning(f\"No refined parameters defined for {name}. Skipping tuning.\")\n",
    "                    continue\n",
    "                \n",
    "                self.logger.info(f\"Refined hyperparameter grid for {name}: {refined_params}\")\n",
    "                \n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=pipeline,\n",
    "                    param_grid=refined_params,\n",
    "                    scoring='f1_weighted',\n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                grid_search.fit(self.X_train, self.y_train)\n",
    "                self.logger.info(f\"{name} Refined Grid Search completed\")\n",
    "                self.logger.info(f\"Best parameters after tuning for {name}: {grid_search.best_params_}\")\n",
    "                self.logger.info(f\"Best cross-validation F1 score after tuning for {name}: {grid_search.best_score_:.3f}\")\n",
    "                \n",
    "                joblib.dump(grid_search.best_estimator_, os.path.join('results', f'{name}_tuned_pipeline.joblib'))\n",
    "                self.logger.info(f\"Saved {name} tuned pipeline to 'results/{name}_tuned_pipeline.joblib'\")\n",
    "                \n",
    "                y_pred = grid_search.predict(self.X_test)\n",
    "                if hasattr(grid_search.best_estimator_.named_steps['classifier'], \"predict_proba\"):\n",
    "                    y_pred_proba = grid_search.predict_proba(self.X_test)\n",
    "                    roc_auc = roc_auc_score(self.y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "                else:\n",
    "                    roc_auc = None\n",
    "                \n",
    "                accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "                \n",
    "                self.logger.info(f\"{name} Tuned Test Accuracy: {accuracy:.3f}\")\n",
    "                self.logger.info(f\"{name} Tuned Test F1 Score: {f1:.3f}\")\n",
    "                if roc_auc is not None:\n",
    "                    self.logger.info(f\"{name} Tuned Test ROC AUC: {roc_auc:.3f}\")\n",
    "                else:\n",
    "                    self.logger.info(f\"{name} Tuned Test ROC AUC: N/A\")\n",
    "                \n",
    "                self.results.append({\n",
    "                    'Model': f\"{name}_Tuned\",\n",
    "                    'CV_F1_Mean': round(grid_search.best_score_, 3),\n",
    "                    'Test_Accuracy': round(accuracy, 3),\n",
    "                    'Test_F1': round(f1, 3),\n",
    "                    'Test_ROC_AUC': round(roc_auc, 3) if roc_auc is not None else None\n",
    "                })\n",
    "                \n",
    "                with open(os.path.join('results', f'{name}_tuned_report.txt'), 'w') as f:\n",
    "                    f.write(f\"Classification Report for {name} Tuned Model:\\n\")\n",
    "                    f.write(classification_report(self.y_test, y_pred, target_names=self.target_encoder.classes_))\n",
    "                    f.write(f\"\\nBest cross-validation F1 score after tuning: {grid_search.best_score_:.3f}\")\n",
    "                    f.write(f\"\\nBest parameters after tuning: {grid_search.best_params_}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error during tuning for {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        tuned_results_df = pd.DataFrame([res for res in self.results if 'Tuned' in res['Model']])\n",
    "        if not tuned_results_df.empty:\n",
    "            tuned_results_df.to_csv(os.path.join('results', 'model_performance_tuned.csv'), index=False)\n",
    "            self.logger.info(\"Tuned model performance metrics saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 01:05:51,395 - INFO - Initializing Academic Dropout Analysis pipeline\n",
      "2024-12-12 01:05:51,431 - INFO - Data loaded successfully with shape: (4424, 37)\n",
      "2024-12-12 01:05:51,432 - INFO - Columns in the dataset: ['marital status', 'application mode', 'application order', 'course', 'daytime/evening attendance', 'previous qualification', 'previous qualification (grade)', 'nacionality', \"mother's qualification\", \"father's qualification\", \"mother's occupation\", \"father's occupation\", 'admission grade', 'displaced', 'educational special needs', 'debtor', 'tuition fees up to date', 'gender', 'scholarship holder', 'age at enrollment', 'international', 'curricular units 1st sem (credited)', 'curricular units 1st sem (enrolled)', 'curricular units 1st sem (evaluations)', 'curricular units 1st sem (approved)', 'curricular units 1st sem (grade)', 'curricular units 1st sem (without evaluations)', 'curricular units 2nd sem (credited)', 'curricular units 2nd sem (enrolled)', 'curricular units 2nd sem (evaluations)', 'curricular units 2nd sem (approved)', 'curricular units 2nd sem (grade)', 'curricular units 2nd sem (without evaluations)', 'unemployment rate', 'inflation rate', 'gdp', 'target']\n",
      "2024-12-12 01:05:51,475 - INFO - Dropped 0 columns with >50% missing values\n",
      "2024-12-12 01:05:51,489 - INFO - Removed 0 constant features\n",
      "2024-12-12 01:05:51,490 - INFO - Starting Exploratory Data Analysis (EDA)\n",
      "2024-12-12 01:06:21,164 - INFO - EDA completed and plots saved in 'eda/' directory\n",
      "2024-12-12 01:06:21,166 - INFO - Starting outlier detection and handling\n",
      "2024-12-12 01:06:21,170 - INFO - Feature 'marital status': Found 126 potential outliers out of 4424 (ratio=0.03)\n",
      "2024-12-12 01:06:21,206 - INFO - Capped outliers in 'marital status' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:21,427 - INFO - Feature 'application mode': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:21,431 - INFO - Feature 'application order': Found 138 potential outliers out of 4424 (ratio=0.03)\n",
      "2024-12-12 01:06:21,459 - INFO - Capped outliers in 'application order' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:21,701 - INFO - Feature 'course': Found 227 potential outliers out of 4424 (ratio=0.05)\n",
      "2024-12-12 01:06:21,708 - INFO - Capped outliers in 'course' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:21,988 - INFO - Feature 'daytime/evening attendance': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:21,992 - INFO - Feature 'previous qualification': Found 308 potential outliers out of 4424 (ratio=0.07)\n",
      "2024-12-12 01:06:21,998 - INFO - Capped outliers in 'previous qualification' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:22,330 - INFO - Feature 'previous qualification (grade)': Found 21 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:22,336 - INFO - Capped outliers in 'previous qualification (grade)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:22,853 - INFO - Feature 'nacionality': Found 74 potential outliers out of 4424 (ratio=0.02)\n",
      "2024-12-12 01:06:22,859 - INFO - Capped outliers in 'nacionality' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:23,175 - INFO - Feature 'mother's qualification': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:23,181 - INFO - Feature 'father's qualification': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:23,184 - INFO - Feature 'mother's occupation': Found 112 potential outliers out of 4424 (ratio=0.03)\n",
      "2024-12-12 01:06:23,189 - INFO - Capped outliers in 'mother's occupation' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:24,008 - INFO - Feature 'father's occupation': Found 177 potential outliers out of 4424 (ratio=0.04)\n",
      "2024-12-12 01:06:24,016 - INFO - Capped outliers in 'father's occupation' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:24,415 - INFO - Feature 'admission grade': Found 22 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:24,421 - INFO - Capped outliers in 'admission grade' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:24,701 - INFO - Feature 'displaced': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:24,705 - INFO - Feature 'educational special needs': Found 51 potential outliers out of 4424 (ratio=0.01)\n",
      "2024-12-12 01:06:24,710 - INFO - Capped outliers in 'educational special needs' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:25,000 - INFO - Feature 'debtor': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:25,005 - INFO - Feature 'tuition fees up to date': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:25,009 - INFO - Feature 'gender': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:25,019 - INFO - Feature 'scholarship holder': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:25,023 - INFO - Feature 'age at enrollment': Found 101 potential outliers out of 4424 (ratio=0.02)\n",
      "2024-12-12 01:06:25,035 - INFO - Capped outliers in 'age at enrollment' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:25,380 - INFO - Feature 'international': Found 110 potential outliers out of 4424 (ratio=0.02)\n",
      "2024-12-12 01:06:25,386 - INFO - Capped outliers in 'international' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:25,637 - INFO - Feature 'curricular units 1st sem (credited)': Found 149 potential outliers out of 4424 (ratio=0.03)\n",
      "2024-12-12 01:06:25,642 - INFO - Capped outliers in 'curricular units 1st sem (credited)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:26,005 - INFO - Feature 'curricular units 1st sem (enrolled)': Found 106 potential outliers out of 4424 (ratio=0.02)\n",
      "2024-12-12 01:06:26,012 - INFO - Capped outliers in 'curricular units 1st sem (enrolled)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:26,449 - INFO - Feature 'curricular units 1st sem (evaluations)': Found 60 potential outliers out of 4424 (ratio=0.01)\n",
      "2024-12-12 01:06:26,454 - INFO - Capped outliers in 'curricular units 1st sem (evaluations)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:26,756 - INFO - Feature 'curricular units 1st sem (approved)': Found 61 potential outliers out of 4424 (ratio=0.01)\n",
      "2024-12-12 01:06:26,763 - INFO - Capped outliers in 'curricular units 1st sem (approved)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:27,160 - INFO - Feature 'curricular units 1st sem (grade)': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:27,167 - INFO - Feature 'curricular units 1st sem (without evaluations)': Found 62 potential outliers out of 4424 (ratio=0.01)\n",
      "2024-12-12 01:06:27,173 - INFO - Capped outliers in 'curricular units 1st sem (without evaluations)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:27,622 - INFO - Feature 'curricular units 2nd sem (credited)': Found 110 potential outliers out of 4424 (ratio=0.02)\n",
      "2024-12-12 01:06:27,667 - INFO - Capped outliers in 'curricular units 2nd sem (credited)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:28,255 - INFO - Feature 'curricular units 2nd sem (enrolled)': Found 82 potential outliers out of 4424 (ratio=0.02)\n",
      "2024-12-12 01:06:28,260 - INFO - Capped outliers in 'curricular units 2nd sem (enrolled)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:28,818 - INFO - Feature 'curricular units 2nd sem (evaluations)': Found 43 potential outliers out of 4424 (ratio=0.01)\n",
      "2024-12-12 01:06:28,824 - INFO - Capped outliers in 'curricular units 2nd sem (evaluations)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:29,351 - INFO - Feature 'curricular units 2nd sem (approved)': Found 23 potential outliers out of 4424 (ratio=0.01)\n",
      "2024-12-12 01:06:29,356 - INFO - Capped outliers in 'curricular units 2nd sem (approved)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:29,867 - INFO - Feature 'curricular units 2nd sem (grade)': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:29,870 - INFO - Feature 'curricular units 2nd sem (without evaluations)': Found 94 potential outliers out of 4424 (ratio=0.02)\n",
      "2024-12-12 01:06:29,875 - INFO - Capped outliers in 'curricular units 2nd sem (without evaluations)' to 1st and 99th percentiles\n",
      "2024-12-12 01:06:30,438 - INFO - Feature 'unemployment rate': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:30,474 - INFO - Feature 'inflation rate': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:30,481 - INFO - Feature 'gdp': Found 0 potential outliers out of 4424 (ratio=0.00)\n",
      "2024-12-12 01:06:30,482 - INFO - Outlier detection and handling completed. Boxplots saved in 'outliers/' directory\n",
      "2024-12-12 01:06:30,486 - INFO - Encoded target classes: ['Dropout' 'Enrolled' 'Graduate']\n",
      "2024-12-12 01:06:30,493 - INFO - Features and target prepared.\n",
      "2024-12-12 01:06:30,506 - INFO - Identified 36 numerical features: ['marital status', 'application mode', 'application order', 'course', 'daytime/evening attendance', 'previous qualification', 'previous qualification (grade)', 'nacionality', \"mother's qualification\", \"father's qualification\", \"mother's occupation\", \"father's occupation\", 'admission grade', 'displaced', 'educational special needs', 'debtor', 'tuition fees up to date', 'gender', 'scholarship holder', 'age at enrollment', 'international', 'curricular units 1st sem (credited)', 'curricular units 1st sem (enrolled)', 'curricular units 1st sem (evaluations)', 'curricular units 1st sem (approved)', 'curricular units 1st sem (grade)', 'curricular units 1st sem (without evaluations)', 'curricular units 2nd sem (credited)', 'curricular units 2nd sem (enrolled)', 'curricular units 2nd sem (evaluations)', 'curricular units 2nd sem (approved)', 'curricular units 2nd sem (grade)', 'curricular units 2nd sem (without evaluations)', 'unemployment rate', 'inflation rate', 'gdp']\n",
      "2024-12-12 01:06:30,507 - INFO - Identified 0 categorical features: []\n",
      "2024-12-12 01:06:30,508 - INFO - Preprocessing pipeline created\n",
      "2024-12-12 01:06:30,556 - INFO - Split data into training ((3539, 36)) and testing ((885, 36)) sets\n",
      "2024-12-12 01:06:30,558 - INFO - \n",
      "Training and evaluating LogisticRegression\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "2024-12-12 01:07:13,676 - INFO - LogisticRegression Grid Search completed\n",
      "2024-12-12 01:07:13,677 - INFO - Best parameters for LogisticRegression: {'classifier__C': 10, 'classifier__penalty': 'l2'}\n",
      "2024-12-12 01:07:13,678 - INFO - Best cross-validation F1 score for LogisticRegression: 0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Evago\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 01:07:13,738 - INFO - Saved LogisticRegression best pipeline\n",
      "2024-12-12 01:07:13,827 - INFO - LogisticRegression Test Accuracy: 0.734\n",
      "2024-12-12 01:07:13,828 - INFO - LogisticRegression Test F1 Score: 0.747\n",
      "2024-12-12 01:07:13,828 - INFO - LogisticRegression Test ROC AUC: 0.889\n",
      "2024-12-12 01:07:13,844 - INFO - \n",
      "Training and evaluating SVC\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data.csv'  \n",
    "delimiter = '\\t'\n",
    "random_state = 42\n",
    "num_features = 36\n",
    "\n",
    "# Initialize the analysis pipeline\n",
    "analysis = AcademicDropoutAnalysis(\n",
    "    data_path=data_path,\n",
    "    delimiter=delimiter,\n",
    "    random_state=random_state,\n",
    "    num_features=num_features\n",
    ")\n",
    "\n",
    "# The constructor automatically performs all steps:\n",
    "# - Data loading\n",
    "# - EDA\n",
    "# - Outlier handling\n",
    "# - Data preparation\n",
    "# - Feature identification\n",
    "# - Preprocessing pipeline creation\n",
    "# - Data splitting\n",
    "# - Model training and evaluation\n",
    "# - Model tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
